
# My TorchLeet Solutions ðŸš€

This repository is a **fork** of [TorchLeet](https://github.com/Exorust/TorchLeet), which I use to track my progress on PyTorch exercises. 

I've added my solutions to these exercises within the "Solutions" folder.

---

## My solutions for TorchLeet [https://github.com/Exorust/TorchLeet]

### ðŸ”µ Basic
Mostly for beginners to get started with PyTorch.

- ~~[Implement linear regression](torch/basic/lin-regression/lin-regression.ipynb) [(Solution)](torch/basic/lin-regression/lin-regression_SOLN.ipynb)~~
- ~~[Write a custom Dataset and Dataloader to load from a CSV file](torch/basic/custom-dataset/custom-dataset.ipynb) [(Solution)](torch/basic/custom-dataset/custom-dataset_SOLN.ipynb)~~
- ~~[Write a custom activation function (Simple)](torch/basic/custom-activation/custom-activation.ipynb) [(Solution)](torch/basic/custom-activation/custom-activation_SOLN.ipynb)~~
- ~~ [Implement Custom Loss Function (Huber Loss)](torch/basic/custom-loss/custom-loss.ipynb) [(Solution)](torch/basic/custom-loss/custom-loss_SOLN.ipynb)~~
- ~~ [Implement a Deep Neural Network](torch/basic/custom-DNN/custon-DNN.ipynb) [(Solution)](torch/basic/custom-DNN/custon-DNN_SOLN.ipynb)~~
- ~~[Visualize Training Progress with TensorBoard in PyTorch](torch/basic/tensorboard/tensorboard.ipynb) ~~[(Solution)](torch/basic/tensorboard/tensorboard_SOLN.ipynb)
- ~~[Save and Load Your PyTorch Model](torch/basic/save-model/save_model.ipynb) [(Solution)](torch/basic/save-model/save_model_SOLN.ipynb)~~
- [ ] Implement Softmax function from scratch

---

### ðŸŸ¢ Easy
Recommended for those who have a basic understanding of PyTorch and want to practice their skills.

- [ ] Implement a CNN on CIFAR-10](torch/easy/cnn/CNN.ipynb) [(Solution)](torch/easy/cnn/CNN_SOLN.ipynb)
- [ ] Implement an RNN from Scratch](torch/easy/rnn/RNN.ipynb) [(Solution)](torch/easy/rnn/RNN_SOLN.ipynb)
- [ ] Use `torchvision.transforms` to apply data augmentation](torch/easy/augmentation/augmentation.ipynb) [(Solution)](torch/easy/augmentation/augmentation_SOLN.ipynb)
- [ ] Add a benchmark to your PyTorch code](torch/easy/benchmark/bench.ipynb) [(Solution)](torch/easy/benchmark/bench_SOLN.ipynb)
- [ ] Train an autoencoder for anomaly detection](torch/easy/autoencoder/autoencoder.ipynb) [(Solution)](torch/easy/autoencoder/autoencoder_SOLN.ipynb)
- [ ] Quantize your language model](torch/easy/quantize-lm/quantize-language-model.ipynb) [(Solution)](torch/easy/quantize-lm/quantize-language-model_SOLN.ipynb)
- [ ] Implement Mixed Precision Training using torch.cuda.amp](torch/easy/cuda-amp/cuda-amp.ipynb) [(Solution)](torch/easy/cuda-amp/cuda-amp_SOLN.ipynb)

---

### ðŸŸ¡ Medium
These problems are designed to challenge your understanding of PyTorch and deep learning concepts. They require you to implement things from scratch or apply advanced techniques.

- [ ] Implement parameter initialization for a CNN](torch/medium/cnn-param-init/CNN_ParamInit.ipynb) [(Solution)](torch/medium/cnn-param-init/CNN_ParamInit_SOLN.ipynb)
- [ ] Implement a CNN from Scratch](torch/medium/cnn-scratch/CNN_scratch.ipynb) [(Solution)](torch/medium/cnn-scratch/CNN_scratch_SOLN.ipynb)
- [ ] Implement an LSTM from Scratch](torch/medium/lstm/LSTM.ipynb) [(Solution)](torch/medium/lstm/LSTM_SOLN.ipynb)
- [ ] Implement AlexNet from scratch
- [ ] Build a Dense Retrieval System using PyTorch
- [ ] Implement KNN from scratch in PyTorch
- [ ] Train a 3D CNN network for segmenting CT images](torch/medium/3dcnn/3DCNN.ipynb) [Solution](torch/medium/3dcnn/3DCNN_SOLN.ipynb)

---

### ðŸ”´ Hard
These problems are for advanced users who want to push their PyTorch skills to the limit. They involve complex architectures, custom layers, and advanced techniques.

- [ ] Write a custom Autograd function for activation (SILU)](torch/hard/custom-autograd/custom-autgrad-function.ipynb) [(Solution)](torch/hard/custom-autograd/custom-autgrad-function_SOLN.ipynb)
- [ ] Write a Neural Style Transfer
- [ ] Build a Graph Neural Network (GNN) from scratch
- [ ] Build a Graph Convolutional Network (GCN) from scratch
- [ ] Write a Transformer](torch/hard/transformer/transformer.ipynb) [(Solution)](torch/hard/transformer/transformer_SOLN.ipynb)
- [ ] Write a GAN](torch/hard/GAN/GAN.ipynb) [(Solution)](torch/hard/GAN/GAN_SOLN.ipynb)
- [ ] Write Sequence-to-Sequence with Attention](torch/hard/seq-seq/seq-to-seq-with-Attention.ipynb) [(Solution)](torch/hard/seq-seq/seq-to-seq-with-Attention_SOLN.ipynb)
- [ ] Enable distributed training in pytorch (DistributedDataParallel)]
- [ ] Work with Sparse Tensors]
- [ ] Add GradCam/SHAP to explain the model.](torch/hard/xai/xai.ipynb) [(Solution)](torch/hard/xai/xai_SOLN.ipynb)
- [ ] Linear Probe on CLIP Features
- [ ] Add Cross Modal Embedding Visualization to CLIP (t-SNE/UMAP)
- [ ] Implement a Vision Transformer
- [ ] Implement a Variational Autoencoder

---

## LLM Set

**An all new set of questions to help you understand and implement Large Language Models from scratch.**

Each question is designed to take you one step closer to building your own LLM.

- [ ] Implement KL Divergence Loss
- [ ] Implement RMS Norm
- [ ] Implement Byte Pair Encoding from Scratch](llm/Byte-Pair-Encoder/BPE-q3.ipynb) [(Solution)](llm/Byte-Pair-Encoder/BPE-q3.ipynb)
- [ ] Create a RAG Search of Embeddings from a set of Reviews
- [ ] Implement Predictive Prefill with Speculative Decoding
- [ ] Implement Attention from Scratch](llm/Implement-Attention-from-Scratch/attention-q4-Question.ipynb) [(Solution)](llm/Implement-Attention-from-Scratch/attention-q4.ipynb)
- [ ] Implement Multi-Head Attention from Scratch](llm/Multi-Head-Attention/multi-head-attention-q5-Question.ipynb) [(Solution)](llm/Multi-Head-Attention/multi-head-attention-q5.ipynb)
- [ ] Implement Grouped Query Attention from Scratch](llm/Grouped-Query-Attention/grouped-query-attention-Question.ipynb) [(Solution)](llm/Grouped-Query-Attention/grouped-query-attention.ipynb)
- [ ] Implement KV Cache in Multi-Head Attention from Scratch
- [ ] Implement Sinusoidal Embeddings](llm/Sinusoidal-Positional-Embedding/sinusoidal-q7-Question.ipynb) [(Solution)](llm/Sinusoidal-Positional-Embedding/sinusoidal-q7.ipynb)
- [ ] Implement ROPE Embeddings](llm/Rotary-Positional-Embedding/rope-q8-Question.ipynb) [(Solution)](llm/Rotary-Positional-Embedding/rope-q8.ipynb)
- [ ] Implement SmolLM from Scratch](llm/SmolLM/smollm-q12-Question.ipynb) [(Solution)](llm/SmolLM/smollm-q12.ipynb)
- [ ] Implement Quantization of Models
    - [ ] GPTQ
- [ ] Implement Beam Search atop LLM for decoding
- [ ] Implement Top K Sampling atop LLM for decoding
- [ ] Implement Top p Sampling atop LLM for decoding
- [ ] Implement Temperature Sampling atop LLM for decoding
- [ ] Implement LoRA on a layer of an LLM
    - [ ] QLoRA
- [ ] Mix two models to create a mixture of Experts
- [ ] Apply SFT on SmolLM
- [ ] Apply RLHF on SmolLM
- [ ] Implement DPO based RLHF
- [ ] Add continuous batching to your LLM
- [ ] Chunk Textual Data for Dense Passage Retrieval
- [ ] Implement Large scale Training => 5D Parallelism

**What's cool? ðŸš€**
- **Diverse Questions**: Covers beginner to advanced PyTorch concepts (e.g., tensors, autograd, CNNs, GANs, and more).
- **Guided Learning**: Includes incomplete code blocks (`...` and `#TODO`) for hands-on practice along with Answers

---

## Getting Started

### 1. Install Dependencies
- Install pytorch: [Install pytorch locally](https://pytorch.org/get-started/locally/)
- Some problems need other packages. Install as needed.

### 2. Structure
- `<E/M/H><ID>/`: Easy/Medium/Hard along with the question ID.
- `<E/M/H><ID>/qname.ipynb`: The question file with incomplete code blocks.
- `<E/M/H><ID>/qname_SOLN.ipynb`: The corresponding solution file.



